{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import HTMLParser as htm\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\"@RevRunWisdom: not afraid of tomorrow, for I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Extreme can neither fight nor fly.\\n-- Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  I got a surprise for all you bitches...pull th...  \n",
       "1       :: joy         1  If I was a thief.. The first thing I would ste...  \n",
       "2      :: fear         0  \"\"@RevRunWisdom: not afraid of tomorrow, for I...  \n",
       "3      :: fear         0  \"Extreme can neither fight nor fly.\\n-- Willia...  \n",
       "4  :: surprise         0  Thinks that @melbahughes had a great 50th birt...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data_1.csv\",sep='\\t',quoting=3)\n",
    "data[\"escape\"] = data.apply(lambda row: htm.HTMLParser().unescape(row[1].decode(\"utf-8\")),axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Converts to lowercase, strips out punctuation,\n",
    "    removes excess whitespace within a string & leading & trailing whitespace\"\"\"\n",
    "    new_list = []\n",
    "    table = string.maketrans(\"\",\"\")\n",
    "    for elem in data:\n",
    "        elem = \"\".join(i for i in elem if ord(i)<128)\n",
    "        elem = str(elem)        \n",
    "        elem = elem.lower()\n",
    "        elem = elem.translate(table, string.punctuation)\n",
    "        elem = re.sub(' +',' ', elem)\n",
    "        elem = elem.strip()\n",
    "        \n",
    "        new_list.append(elem)\n",
    "    return new_list\n",
    "\n",
    "#train_pol_x = process_data(train_pol_x)\n",
    "#test_pol_x = process_data(test_pol_x)\n",
    "\n",
    "#Clean entire data set at once\n",
    "data.escape = process_data(data.escape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test data frames\n",
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "\n",
    "# Train and test target labels for polarity\n",
    "train_pol_y = train.ix[:,3].tolist()\n",
    "test_pol_y = test.ix[:,3].tolist()\n",
    "\n",
    "# Binarize labels for sub-emotion classifier\n",
    "train_emo = train.ix[:,2].tolist()\n",
    "test_emo = test.ix[:,2].tolist()\n",
    "emo_bin = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Labels for sub-emotion classifier\n",
    "train_emo_y = emo_bin.fit_transform(train_emo)\n",
    "tests_emo_y = emo_bin.transform(test_emo)\n",
    "\n",
    "# Train and test inputs\n",
    "train_pol_x = train.ix[:, 4].tolist()\n",
    "test_pol_x = test.ix[:, 4].tolist()\n",
    "\n",
    "\n",
    "#save data to recall later\n",
    "#np.savez('train_test.npz', train_pol_y=train_pol_y, test_pol_y=test_pol_y,train_pol_x=train_pol_x,\\\n",
    "        #test_pol_x=test_pol_x, train_emo=train_emo,test_emo=test_emo,train_emo_y=train_emo_y,\\\n",
    "        #tests_emo_y=tests_emo_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Pull in word list & vectors\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) #Maximum number of words in a tweet\n",
    "# numDimensions = 25 #Dimensions for each word vector\n",
    "\n",
    "def get_matrix_ids(data, maxSeqLength):\n",
    "    numFiles = len(data)\n",
    "    ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "\n",
    "    for fileCounter, tweet in enumerate(data):\n",
    "        start = time.time()\n",
    "        split = tweet.split()\n",
    "        for indexCounter, word in enumerate(split):\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        if fileCounter % 500 == 0:\n",
    "            print \"Tweet matrices completed:\", fileCounter\n",
    "    end = time.time()\n",
    "    print \"Time elapsed\", (end - start)\n",
    "    return ids\n",
    "\n",
    "\n",
    "#train_ids = get_matrix_ids(train_pol_x, maxSeqLength)\n",
    "#test_ids = get_matrix_ids(test_pol_x, maxSeqLength)\n",
    "#np.savez('ids.npz', train_ids=train_ids, test_ids=test_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load all of train and test data\n",
    "p = np.load('train_test.npz')\n",
    "train_pol_y = p['train_pol_y']\n",
    "test_pol_y = p['test_pol_y']\n",
    "train_pol_x = p['train_pol_x']\n",
    "test_pol_x = p['test_pol_x']\n",
    "train_emo = p['train_emo']\n",
    "test_emo = p['test_emo']\n",
    "train_emo_y = p['train_emo_y']\n",
    "tests_emo_y = p['tests_emo_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get matrix ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix ids for each tweet were built using GloVe word embeddings\n",
    "# Because construction of matrix ids is computationally expensive,\n",
    "# matrix ids were saved and will simply be reloaded\n",
    "d = np.load('ids.npz')\n",
    "train_ids = d['train_ids']\n",
    "test_ids = d['test_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16840, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      ":: sadness\n",
      "[ 8254  3711     4  1147   373     4 74927     7  4249   539  1720     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n",
      "boyfriend flew to chicago today to appraise a comic book collection\n",
      "(16840, 31)\n",
      "(16840, 32)\n"
     ]
    }
   ],
   "source": [
    "#Add extra dimension to traim_ids_emo for polarity\n",
    "\n",
    "print train_pol_y[0]\n",
    "print train_emo[0]\n",
    "print train_ids[0]\n",
    "\n",
    "pol_array = np.asarray(train_pol_y).reshape(-1,1)\n",
    "train_ids_emo = np.append(train_ids, pol_array,axis=1)\n",
    "print train_pol_x[0]\n",
    "print train_ids.shape\n",
    "print train_ids_emo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# For Polarity Classifier\n",
    "def getTrainBatch(train_data, train_labels, train_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        if train_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def getTestBatch(test_data, test_labels, test_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "        if test_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "# For sub-emotion classifier\n",
    "def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize-10): #took out -5\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        labels.append(train_labels[num-1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "    \n",
    "    disgust = []\n",
    "    for m in range(len(train_labels)):\n",
    "        if train_labels[m][1] == 1:\n",
    "            disgust.append(m)\n",
    "    \n",
    "    for mel in range(5):\n",
    "        num = randint(1, (len(disgust)-1))\n",
    "        ind = disgust[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[batchSize-mel-1] = train_ids[ind]\n",
    "        \n",
    "    anger = []\n",
    "    for p in range(len(train_labels)):\n",
    "        if train_labels[p][0] == 1:\n",
    "            anger.append(p)\n",
    "    \n",
    "    for pri in range(5,10):\n",
    "        num = randint(1, (len(anger)-1))\n",
    "        ind = anger[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[batchSize-pri-1] = train_ids[ind]\n",
    "    \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "\n",
    "def getTestBatch_subEmo(test_data, test_labels, test_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        labels.append(test_labels[num-1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def matmul3d(X, W):\n",
    "    \"\"\"Wrapper for tf.matmul to handle a 3D input tensor X.\n",
    "    Will perform multiplication along the last dimension.\n",
    "    Args:\n",
    "      X: [m,n,k]\n",
    "      W: [k,l]\n",
    "    Returns:\n",
    "      XW: [m,n,l]\n",
    "    \"\"\"\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)\n",
    "\n",
    "\n",
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "#     cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)])\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 34, 55, 63, 88]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger = []\n",
    "for p in range(len(train_emo_y)):\n",
    "    if train_emo_y[p][0] == 1:\n",
    "        anger.append(p)\n",
    "        \n",
    "print anger[:5]\n",
    "\n",
    "train_emo_y[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Perc': {':: anger': 23.02215189873418,\n",
       "  ':: disgust': 24.34640522875817,\n",
       "  ':: fear': 24.217026907807675,\n",
       "  ':: joy': 24.981040497497347,\n",
       "  ':: sadness': 26.73726009265387,\n",
       "  ':: surprise': 24.886437378325763},\n",
       " 'TEST': {':: anger': 291,\n",
       "  ':: disgust': 149,\n",
       "  ':: fear': 549,\n",
       "  ':: joy': 1647,\n",
       "  ':: sadness': 808,\n",
       "  ':: surprise': 767},\n",
       " 'TRAIN': {':: anger': 1264,\n",
       "  ':: disgust': 612,\n",
       "  ':: fear': 2267,\n",
       "  ':: joy': 6593,\n",
       "  ':: sadness': 3022,\n",
       "  ':: surprise': 3082}}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check train data first\n",
    "check = np.unique(train_emo, return_counts=True)\n",
    "check_test = np.unique(test_emo, return_counts=True)\n",
    "\n",
    "counts = {\"TRAIN\":{},\"TEST\":{},\"Perc\":{}}\n",
    "for i in range(6):\n",
    "    counts[\"TRAIN\"][check[0][i]] = check[1][i]\n",
    "    counts[\"TEST\"][check_test[0][i]] = check_test[1][i]\n",
    "    counts[\"Perc\"][check[0][i]]= float(check_test[1][i])/float(check[1][i])*100\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check batch data\n",
    "def check_bal(nextBatchLabels):\n",
    "    x = np.asarray(nextBatchLabels)\n",
    "    x2 = np.unique(x,return_counts=True,axis=0)\n",
    "    labs = emo_bin.inverse_transform(x2[0])\n",
    "    for i in range(6):\n",
    "        print x2[0][i],labs[i],x2[1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1] :: surprise 28\n",
      "[0 0 0 0 1 0] :: sadness 22\n",
      "[0 0 0 1 0 0] :: joy 57\n",
      "[0 0 1 0 0 0] :: fear 15\n",
      "[0 1 0 0 0 0] :: disgust 13\n",
      "[1 0 0 0 0 0] :: anger 15\n"
     ]
    }
   ],
   "source": [
    "nextBatch, nextBatchLabels = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids_emo, batchSize, maxSeqLength)\n",
    "check_bal(nextBatchLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity Classifier  w/ Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.79      0.85      0.82      2564\n",
      "   Positive       0.74      0.66      0.70      1647\n",
      "\n",
      "avg / total       0.77      0.78      0.77      4211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 2,\n",
    "                            max_df=.5,\n",
    "                            use_idf=True,\n",
    "                            stop_words='english',\n",
    "                            sublinear_tf=True\n",
    "                             \n",
    "                            )\n",
    "train_vectors = vectorizer.fit_transform(train_pol_x)\n",
    "test_vectors = vectorizer.transform(test_pol_x)\n",
    "\n",
    "base1 = svm.LinearSVC(loss=\"hinge\")#svm.SVC(kernel='linear')\n",
    "base1.fit(train_vectors, train_pol_y)\n",
    "predict_base1 = base1.predict(test_vectors)\n",
    "\n",
    "target_names = [\"Negative\",\"Positive\"]\n",
    "print classification_report(test_pol_y,predict_base1, target_names = target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save as npz\n",
    "#np.savez('pol_predictions.npz', sci_svm=predict_base1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4211, 32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call in npz labels\n",
    "m = np.load('pol_predictions.npz')\n",
    "predicted_svm = m['sci_svm']\n",
    "\n",
    "#add predicted labels as [32] into test_ids \n",
    "predict_pol = predicted_svm.reshape(-1,1)\n",
    "test_ids_emo = np.append(test_ids, predict_pol,axis=1)\n",
    "\n",
    "test_ids_emo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "\n",
    "Changes: \n",
    "- adding forget_bias to the LSTM Cell\n",
    "- adding keep_prob\n",
    "\n",
    "Check on:\n",
    "- use of tf.nn.dynamic_rnn cell vs MultiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Output_Layer/Add:0\", shape=(150, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "\n",
    "#7/30 added 1 to increase max length to add a polarity field\n",
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]])+1 #Maximum number of words in a tweet\n",
    "batchSize = 150\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.001\n",
    "\n",
    "iterations = 1500\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "# Lookup word vectors\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "#     print \"Embedding Layer shape\", data_vec.shape\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=0.0)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)            \n",
    "    lstmCell = tf.contrib.rnn.MultiRNNCell([lstmCell] * hiddenStateSize)\n",
    "    value, _ = tf.nn.dynamic_rnn(lstmCell, data_vec, dtype=tf.float32)\n",
    "#     print \"Output of RNN shape\", value.shape\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    weight = tf.Variable(tf.random_uniform([numDimensions, numClasses], -1.0, 1.0))\n",
    "    bias = tf.zeros(numClasses, tf.float32)\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    multiplier = tf.matmul(last, weight)\n",
    "    prediction = tf.add(multiplier, bias)\n",
    "    print prediction\n",
    " \n",
    "#     print \"Weights shape\", weight.shape\n",
    "#     print \"Bias shape\", bias.shape\n",
    "#     print \"New shape for value\", value.shape\n",
    "#     print \"last shape\", last.shape\n",
    "#     print \"multiplier shape\", multiplier.shape\n",
    "#     print \"Output shape\", prediction.shape\n",
    "\n",
    "    \n",
    "# From A3\n",
    "#     multiplier = matmul3d(value, weight)\n",
    "#     print \"Multiplier shape\", multiplier.shape\n",
    "#     prediction = tf.add(multiplier, bias)\n",
    "#     print \"Logits shape\", prediction.shape\n",
    "    \n",
    "with tf.name_scope(\"Prediction_Layer\"):\n",
    "    # Define correct predictions and accuracy\n",
    "    comparison = tf.argmax(prediction,1)\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids_emo, batchSize, maxSeqLength);\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    # Write summary to Tensorboard\n",
    "    summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    writer.add_summary(summary, i)\n",
    "\n",
    "#     # Save the network every 10,000 training iterations\n",
    "#     if (i % 10000 == 0 and i != 0):\n",
    "#         save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#         print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 400\n",
    "l_predictions = []\n",
    "l_labels = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch_subEmo(test_pol_x, tests_emo_y, test_ids_emo, batchSize, maxSeqLength)\n",
    "\n",
    "    p, q= (sess.run([comparison,accuracy], {input_data: nextBatch, labels: nextBatchLabels}))\n",
    "    l_predictions.append(p)\n",
    "    l_labels.append(nextBatchLabels)\n",
    "    #print(\"Accuracy for this batch:\",q)\n",
    "\n",
    "    #print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   :: anger       0.18      0.26      0.21      4334\n",
      " :: disgust       0.05      0.00      0.01      2133\n",
      "    :: fear       0.46      0.42      0.44      7730\n",
      "     :: joy       0.73      0.65      0.69     23193\n",
      " :: sadness       0.30      0.43      0.35     11535\n",
      ":: surprise       0.43      0.38      0.40     11075\n",
      "\n",
      "avg / total       0.49      0.48      0.48     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = emo_bin.classes_.tolist()\n",
    "def score(preds,labels,target_names):\n",
    "    predictions = np.asarray(preds).ravel()\n",
    "    labels = np.argmax(np.asarray(labels),2).ravel()\n",
    "    \n",
    "    print classification_report(labels,predictions,target_names=target_names)\n",
    "    \n",
    "score(l_predictions,l_labels,target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
