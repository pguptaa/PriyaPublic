{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import HTMLParser as htm\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and preprocess/clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\"@RevRunWisdom: not afraid of tomorrow, for I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Extreme can neither fight nor fly.\\n-- Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  I got a surprise for all you bitches...pull th...  \n",
       "1       :: joy         1  If I was a thief.. The first thing I would ste...  \n",
       "2      :: fear         0  \"\"@RevRunWisdom: not afraid of tomorrow, for I...  \n",
       "3      :: fear         0  \"Extreme can neither fight nor fly.\\n-- Willia...  \n",
       "4  :: surprise         0  Thinks that @melbahughes had a great 50th birt...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweet_data_1.csv\",sep='\\t',quoting=3)\n",
    "data[\"escape\"] = data.apply(lambda row: htm.HTMLParser().unescape(row[1].decode(\"utf-8\")),axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Positive</th>\n",
       "      <th>escape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138881940341260288:</td>\n",
       "      <td>I got a surprise for all you bitches...pull th...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>i got a surprise for all you bitchespull theri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144479819843911683:</td>\n",
       "      <td>If I was a thief.. The first thing I would ste...</td>\n",
       "      <td>:: joy</td>\n",
       "      <td>1</td>\n",
       "      <td>if i was a thief the first thing i would steal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139110849120972800:</td>\n",
       "      <td>\"&amp;quot;@RevRunWisdom: not afraid of tomorrow, ...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>revrunwisdom not afraid of tomorrow for i have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141532076791971840:</td>\n",
       "      <td>\"Extreme can neither fight nor fly.&amp;#xA;-- Wil...</td>\n",
       "      <td>:: fear</td>\n",
       "      <td>0</td>\n",
       "      <td>extreme can neither fight nor fly\\n william sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145353048817012736:</td>\n",
       "      <td>Thinks that @melbahughes had a great 50th birt...</td>\n",
       "      <td>:: surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>thinks that melbahughes had a great 50th birth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id                                              Tweet  \\\n",
       "0  138881940341260288:  I got a surprise for all you bitches...pull th...   \n",
       "1  144479819843911683:  If I was a thief.. The first thing I would ste...   \n",
       "2  139110849120972800:  \"&quot;@RevRunWisdom: not afraid of tomorrow, ...   \n",
       "3  141532076791971840:  \"Extreme can neither fight nor fly.&#xA;-- Wil...   \n",
       "4  145353048817012736:  Thinks that @melbahughes had a great 50th birt...   \n",
       "\n",
       "       Emotion  Positive                                             escape  \n",
       "0  :: surprise         0  i got a surprise for all you bitchespull theri...  \n",
       "1       :: joy         1  if i was a thief the first thing i would steal...  \n",
       "2      :: fear         0  revrunwisdom not afraid of tomorrow for i have...  \n",
       "3      :: fear         0  extreme can neither fight nor fly\\n william sh...  \n",
       "4  :: surprise         0  thinks that melbahughes had a great 50th birth...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Converts to lowercase, strips out punctuation,\n",
    "    removes excess whitespace within a string & leading & trailing whitespace\"\"\"\n",
    "    new_list = []\n",
    "    table = string.maketrans(\"\",\"\")\n",
    "    for elem in data:\n",
    "        elem = \"\".join(i for i in elem if ord(i)<128)\n",
    "        elem = str(elem)        \n",
    "        elem = elem.lower()\n",
    "        # New addition to handle elipsis\n",
    "#         elem = re.sub('\\\\.+', ' ', elem)\n",
    "        elem = elem.translate(table, string.punctuation)\n",
    "        elem = re.sub(' +',' ', elem)\n",
    "        elem = elem.strip()\n",
    "        \n",
    "        new_list.append(elem)\n",
    "    return new_list\n",
    "\n",
    "#Clean entire data set at once\n",
    "data.escape = process_data(data.escape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "# Pull in word list & vectors\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load all of train and test data\n",
    "p = np.load('train_test.npz')\n",
    "train_pol_y = p['train_pol_y']\n",
    "test_pol_y = p['test_pol_y']\n",
    "train_pol_x = p['train_pol_x']\n",
    "test_pol_x = p['test_pol_x']\n",
    "train_emo = p['train_emo']\n",
    "test_emo = p['test_emo']\n",
    "train_emo_y = p['train_emo_y']\n",
    "tests_emo_y = p['tests_emo_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get matrix ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix ids for each tweet were built using GloVe word embeddings\n",
    "# Because construction of matrix ids is computationally expensive,\n",
    "# matrix ids were saved and will simply be reloaded\n",
    "d = np.load('ids.npz')\n",
    "train_ids = d['train_ids']\n",
    "test_ids = d['test_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16840, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import random\n",
    "\n",
    "# For Polarity Classifier\n",
    "def getTrainBatch(train_data, train_labels, train_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1, (len(train_data)-1))\n",
    "        if train_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = train_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "def getTestBatch(test_data, test_labels, test_ids):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "        if test_labels[num-1] == 1:\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "            \n",
    "        arr[i] = test_ids[num-1:num]\n",
    "        \n",
    "    return arr.astype(int), labels\n",
    "\n",
    "\n",
    "\n",
    "def getTrainBatch_subEmo(train_data, train_labels, train_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    inds = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    # iterate through batch size\n",
    "    #for i in range(batchSize-10): \n",
    "        #num = randint(1, (len(train_data)-1))\n",
    "        \n",
    "    count = 0\n",
    "    for num in random.sample(xrange(1,(len(train_data)-1)), batchSize-10):\n",
    "        labels.append(train_labels[num-1])\n",
    "            \n",
    "        #arr[i]    \n",
    "        arr[count] = train_ids[num-1:num]\n",
    "        inds.append(num-1)\n",
    "        count +=1\n",
    "        \n",
    "    disgust = []\n",
    "    for m in range(len(train_labels)):\n",
    "        if train_labels[m][1] == 1:\n",
    "            disgust.append(m)\n",
    "    \n",
    "    #for mel in range(5):\n",
    "        #num = randint(1, (len(disgust)-1))\n",
    "    for num in random.sample(xrange(1,(len(disgust)-1)), 5):\n",
    "        ind = disgust[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[count] = train_ids[ind]\n",
    "        inds.append(ind)\n",
    "        count +=1\n",
    "        \n",
    "    anger = []\n",
    "    for p in range(len(train_labels)):\n",
    "        if train_labels[p][0] == 1:\n",
    "            anger.append(p)\n",
    "    \n",
    "    #for pri in range(5,10):\n",
    "        #num = randint(1, (len(anger)-1))\n",
    "    for num in random.sample(xrange(1,(len(anger)-1)), 5):\n",
    "        ind = anger[num]\n",
    "        labels.append(train_labels[ind])\n",
    "        arr[count] = train_ids[ind]\n",
    "        inds.append(ind)\n",
    "        count +=1\n",
    "    \n",
    "    return arr.astype(int), labels,inds\n",
    "\n",
    "\n",
    "def getTestBatch_subEmo(test_data, test_labels, test_ids, batchSize, maxSeqLength):\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    inds=[]\n",
    "    #for i in range(batchSize):\n",
    "        #num = randint(1,(len(test_data)-1))\n",
    "        \n",
    "    count = 0\n",
    "    for num in random.sample(xrange(1,(len(test_data)-1)), batchSize):\n",
    "        labels.append(test_labels[num-1])\n",
    "            \n",
    "        arr[count] = test_ids[num-1:num]\n",
    "        inds.append(num-1)\n",
    "        count +=1\n",
    "        \n",
    "    return arr.astype(int), labels,inds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-emotion Classifier without polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "\n",
    "maxSeqLength = max([len(elem.split()) for elem in data.ix[:, 4]]) #Maximum number of words in a tweet\n",
    "batchSize = 150\n",
    "hiddenStateSize = 1\n",
    "# lstmUnits = 2\n",
    "numClasses = 6\n",
    "numDimensions = 50\n",
    "keepProb = 0.5\n",
    "learningRate = 0.001\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "# Reset graph & create placeholders\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "ns = tf.tile([maxSeqLength], [batchSize, ])\n",
    "\n",
    "# Lookup word vectors\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    data_vec = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "    data_vec = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Cell_RNN_Layer\"):\n",
    "    cells=[]\n",
    "    for _ in range(hiddenStateSize):\n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(numDimensions, forget_bias=1.0)\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, input_keep_prob=keepProb, output_keep_prob=keepProb)        \n",
    "        cells.append(lstmCell)\n",
    "        multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    value, _ = tf.nn.dynamic_rnn(multicell, data_vec, sequence_length=ns, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    weight = tf.Variable(tf.random_uniform([numDimensions, numClasses], -1.0, 1.0))\n",
    "    bias = tf.Variable(tf.random_uniform([numClasses], -1.0, 1.0))\n",
    "    value = tf.transpose(value, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "    multiplier = tf.matmul(last, weight)\n",
    "    prediction = tf.add(multiplier, bias)\n",
    "\n",
    "#     print \"Embedding Layer shape\", data_vec.shape\n",
    "#     print \"Output of RNN shape\", value.shape\n",
    "#     print \"Weights shape\", weight.shape\n",
    "#     print \"Bias shape\", bias.shape\n",
    "#     print \"New shape for value\", value.shape\n",
    "#     print \"last shape\", last.shape\n",
    "#     print \"multiplier shape\", multiplier.shape\n",
    "#     print \"Output shape\", prediction.shape\n",
    "    \n",
    "with tf.name_scope(\"Prediction_Layer\"):\n",
    "    # Define correct predictions and accuracy\n",
    "    comparison = tf.argmax(prediction,1)\n",
    "    correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "    # Define loss & optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_inds = []\n",
    "train_logits = []\n",
    "train_labels = []\n",
    "for i in range(iterations):\n",
    "    # Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels,train_i = getTrainBatch_subEmo(train_pol_x, train_emo_y, train_ids, batchSize, maxSeqLength)\n",
    "    train_inds.append(train_i)\n",
    "    train_logs = sess.run([prediction,optimizer], {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    train_logits.append(train_logs[0])\n",
    "    train_labels.append(nextBatchLabels)\n",
    "    # Write summary to Tensorboard\n",
    "    summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "    writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "l_predictions = []\n",
    "l_labels = []\n",
    "l_logits = []\n",
    "l_inds = []\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels,test_i = getTestBatch_subEmo(test_pol_x, tests_emo_y, test_ids, batchSize, maxSeqLength)\n",
    "\n",
    "    test_log,p,q= (sess.run([prediction,comparison,accuracy], {input_data: nextBatch, labels: nextBatchLabels}))\n",
    "    l_predictions.append(p)\n",
    "    l_labels.append(nextBatchLabels)\n",
    "    l_logits.append(test_log)\n",
    "    l_inds.append(test_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   :: anger       0.22      0.22      0.22      5205\n",
      " :: disgust       0.15      0.07      0.10      2594\n",
      "    :: fear       0.46      0.39      0.42      9703\n",
      "     :: joy       0.52      0.81      0.63     29165\n",
      " :: sadness       0.30      0.15      0.20     14546\n",
      ":: surprise       0.49      0.27      0.34     13787\n",
      "\n",
      "avg / total       0.43      0.46      0.42     75000\n",
      "\n",
      "(5, 1) 101\n",
      "(1, 2) 151\n",
      "(1, 0) 167\n",
      "(1, 5) 204\n",
      "(5, 0) 245\n",
      "(2, 1) 275\n",
      "(1, 4) 279\n",
      "(1, 3) 281\n",
      "(4, 1) 451\n",
      "(0, 1) 494\n",
      "(0, 5) 538\n",
      "(5, 2) 641\n",
      "(2, 0) 681\n",
      "(4, 0) 693\n",
      "(2, 5) 737\n",
      "(4, 2) 828\n",
      "(0, 3) 917\n",
      "(0, 4) 1020\n",
      "(3, 1) 1080\n",
      "(0, 2) 1211\n",
      "(2, 4) 1313\n",
      "(2, 3) 1347\n",
      "(5, 3) 1439\n",
      "(5, 4) 1456\n",
      "(4, 5) 1571\n",
      "(4, 3) 1689\n",
      "(3, 0) 2263\n",
      "(3, 2) 3127\n",
      "(3, 5) 7063\n",
      "(3, 4) 8265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "# target_names = emo_bin.classes_.tolist()\n",
    "target_names = [':: anger', ':: disgust', ':: fear', ':: joy', ':: sadness', ':: surprise']\n",
    "def score(preds,labels,target_names, indexes):\n",
    "    predictions = np.asarray(preds).ravel()\n",
    "    labels = np.argmax(np.asarray(labels),2).ravel()\n",
    "    indexes = np.asarray(indexes).ravel()\n",
    "    \n",
    "    print classification_report(labels,predictions,target_names=target_names)\n",
    "    \n",
    "    errors = dict()\n",
    "    examples = dict()\n",
    "    for i, p in enumerate(predictions):\n",
    "        if p != labels[i]:\n",
    "            if (p, labels[i]) not in errors:\n",
    "                errors[(p, labels[i])] = 1\n",
    "                examples[(p, labels[i])] = [indexes[i]]\n",
    "            else:\n",
    "                errors[(p, labels[i])] += 1 \n",
    "                examples[(p, labels[i])].append(indexes[i])\n",
    "                \n",
    "    return OrderedDict(sorted(errors.items(), key=itemgetter(1))), examples\n",
    "err, ex = score(l_predictions,l_labels,target_names,l_inds)\n",
    "\n",
    "# See which pairs are getting confused most often\n",
    "for key, val in err.iteritems():\n",
    "    print key, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
